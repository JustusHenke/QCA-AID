# Anleitung: Lokale Modelle (LM Studio / Ollama) in der Webapp verwenden

## √úbersicht

Die QCA-AID Webapp unterst√ºtzt jetzt die Verwendung lokaler LLM-Modelle √ºber:
- **LM Studio** (Port 1234)
- **Ollama** (Port 11434)

## Vorteile lokaler Modelle

‚úÖ **Kostenlos** - Keine API-Kosten
‚úÖ **Privat** - Daten bleiben auf Ihrem Computer
‚úÖ **Offline** - Keine Internetverbindung erforderlich
‚úÖ **Kontrolle** - Volle Kontrolle √ºber das Modell

## Schritt-f√ºr-Schritt-Anleitung

### 1. LM Studio oder Ollama installieren

#### Option A: LM Studio (Empfohlen f√ºr Einsteiger)

1. **Download:** [https://lmstudio.ai/](https://lmstudio.ai/)
2. **Installation:** Installieren Sie LM Studio
3. **Modell herunterladen:**
   - √ñffnen Sie LM Studio
   - Gehen Sie zum "Discover" Tab
   - Suchen Sie nach einem Modell (z.B. "Llama 3.1 8B")
   - Klicken Sie auf "Download"
4. **Server starten:**
   - Gehen Sie zum "Local Server" Tab
   - W√§hlen Sie das heruntergeladene Modell
   - Klicken Sie auf "Start Server"
   - Server l√§uft auf Port 1234

#### Option B: Ollama (F√ºr fortgeschrittene Nutzer)

1. **Download:** [https://ollama.ai/](https://ollama.ai/)
2. **Installation:** Installieren Sie Ollama
3. **Modell herunterladen:**
   ```bash
   ollama pull llama3.1:8b
   ```
4. **Server l√§uft automatisch** auf Port 11434

### 2. Modell in der Webapp ausw√§hlen

1. **√ñffnen Sie die QCA-AID Webapp**
   ```bash
   python start_webapp.py
   ```

2. **Gehen Sie zum Konfiguration-Tab**

3. **W√§hlen Sie "Local (LM Studio/Ollama)" als Modell-Anbieter**
   ```
   Modell-Anbieter: Local (LM Studio/Ollama)
   ```

4. **Klicken Sie auf "üîÑ Lokale Modelle erkennen"**
   - Die Webapp sucht nach laufenden Servern
   - Gefundene Modelle werden angezeigt

5. **W√§hlen Sie ein erkanntes Modell aus**
   ```
   Modell-Name: [Ihr Modell]
   ```

6. **Speichern Sie die Konfiguration**

### 3. Analyse starten

Jetzt k√∂nnen Sie Ihre Analyse wie gewohnt starten. Das lokale Modell wird verwendet!

## Empfohlene Modelle

### F√ºr QCA-AID geeignete Modelle:

| Modell | Gr√∂√üe | RAM | Geschwindigkeit | Qualit√§t |
|--------|-------|-----|-----------------|----------|
| **Llama 3.1 8B** | 4.7 GB | 8 GB | ‚ö°‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê Gut |
| **Llama 3.1 70B** | 40 GB | 64 GB | ‚ö° Langsam | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent |
| **Mistral 7B** | 4.1 GB | 8 GB | ‚ö°‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê Gut |
| **Qwen 2.5 14B** | 8.5 GB | 16 GB | ‚ö°‚ö° Mittel | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut |

**Empfehlung f√ºr Einsteiger:** Llama 3.1 8B (gute Balance aus Geschwindigkeit und Qualit√§t)

## Fehlerbehebung

### Problem: "Keine lokalen Modelle gefunden"

**L√∂sung:**
1. Pr√ºfen Sie, ob LM Studio/Ollama l√§uft
2. Pr√ºfen Sie, ob ein Modell geladen ist
3. Pr√ºfen Sie die Ports:
   - LM Studio: http://localhost:1234
   - Ollama: http://localhost:11434

### Problem: "Fehler bei der Erkennung"

**L√∂sung:**
1. Starten Sie LM Studio/Ollama neu
2. Starten Sie die Webapp neu
3. Pr√ºfen Sie die Firewall-Einstellungen

### Problem: Modell ist sehr langsam

**L√∂sung:**
1. Verwenden Sie ein kleineres Modell (z.B. 7B statt 70B)
2. Pr√ºfen Sie, ob Ihr Computer genug RAM hat
3. Schlie√üen Sie andere Programme
4. Verwenden Sie GPU-Beschleunigung (falls verf√ºgbar)

### Problem: Modell gibt schlechte Ergebnisse

**L√∂sung:**
1. Verwenden Sie ein gr√∂√üeres/besseres Modell
2. Passen Sie die Temperatur in den Coder-Einstellungen an
3. Verbessern Sie Ihre Kategoriendefinitionen im Codebook
4. Erw√§gen Sie die Verwendung eines kommerziellen Modells (OpenAI, Anthropic)

## Vergleich: Lokal vs. Cloud

| Aspekt | Lokale Modelle | Cloud-Modelle (OpenAI, etc.) |
|--------|----------------|------------------------------|
| **Kosten** | Kostenlos | $0.15 - $30 pro 1M Tokens |
| **Geschwindigkeit** | Abh√§ngig von Hardware | Sehr schnell |
| **Qualit√§t** | Gut bis sehr gut | Exzellent |
| **Privatsph√§re** | 100% privat | Daten werden verarbeitet |
| **Offline** | Ja | Nein |
| **Setup** | Komplex | Einfach (nur API-Key) |

## Best Practices

### F√ºr optimale Ergebnisse mit lokalen Modellen:

1. **Verwenden Sie pr√§zise Kategoriendefinitionen**
   - Lokale Modelle ben√∂tigen klarere Anweisungen
   - Geben Sie mehr Beispiele im Codebook

2. **Passen Sie die Chunk-Gr√∂√üe an**
   - Kleinere Chunks (500-800 Zeichen) f√ºr kleinere Modelle
   - Gr√∂√üere Chunks (1000-1500 Zeichen) f√ºr gr√∂√üere Modelle

3. **Nutzen Sie Batch-Verarbeitung**
   - Kleinere Batch-Gr√∂√üen (3-5) f√ºr lokale Modelle
   - Verhindert √úberlastung des Modells

4. **Testen Sie verschiedene Modelle**
   - Jedes Modell hat St√§rken und Schw√§chen
   - Testen Sie mit einer kleinen Stichprobe

## Technische Details

### LM Studio API

- **Endpoint:** http://localhost:1234/v1/models
- **Format:** OpenAI-kompatibel
- **Dokumentation:** [LM Studio Docs](https://lmstudio.ai/docs)

### Ollama API

- **Endpoint:** http://localhost:11434/api/tags
- **Format:** Ollama-spezifisch
- **Dokumentation:** [Ollama Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)

### Automatische Erkennung

Die Webapp verwendet den `LocalDetector` aus `QCA_AID_assets/utils/llm/local_detector.py`:

1. **Pr√ºft LM Studio** (Port 1234)
2. **Pr√ºft Ollama API** (Port 11434)
3. **Fallback: Ollama CLI** (`ollama list`)
4. **Gibt Liste aller gefundenen Modelle zur√ºck**

## Weitere Ressourcen

- **LM Studio:** [https://lmstudio.ai/](https://lmstudio.ai/)
- **Ollama:** [https://ollama.ai/](https://ollama.ai/)
- **Modell-√úbersicht:** [https://huggingface.co/models](https://huggingface.co/models)
- **QCA-AID Dokumentation:** [README.md](README.md)

---

**Viel Erfolg mit lokalen Modellen!** üöÄ
