"""
Relevanz-Pr√ºfung f√ºr QCA-AID
=============================
Zentrale Klasse f√ºr Relevanzpr√ºfungen mit Caching und Batch-Verarbeitung.
"""

import json
from ..utils.tracking.token_tracker import TokenTracker, get_global_token_counter
from ..utils.llm.response import LLMResponse
from ..utils.llm.factory import LLMProviderFactory
import time
import asyncio
from typing import List, Tuple, Dict, Optional, Any

from ..core.config import CONFIG, FORSCHUNGSFRAGE, KODIERREGELN
from ..core.data_models import CategoryDefinition
from ..QCA_Prompts import QCAPrompts

# Verwende globale Token-Counter Instanz
token_counter = get_global_token_counter()


class RelevanceChecker:
    """
    Zentrale Klasse f√ºr Relevanzpr√ºfungen mit Caching und Batch-Verarbeitung.
    Reduziert API-Calls durch Zusammenfassung mehrerer Segmente.
    """
    
    def __init__(self, model_name: str, batch_size: int = 5, temperature: float = 0.3):
        self.model_name = model_name
        self.batch_size = batch_size
        self.temperature = float(temperature)

        # Hole Provider aus CONFIG
        provider_name = CONFIG.get('MODEL_PROVIDER', 'openai').lower()  # Fallback zu OpenAI
        try:
            # Wir lassen den LLMProvider sich selbst um die Client-Initialisierung k√ºmmern
            # √úbergebe model_name f√ºr Capability-Testing
            self.llm_provider = LLMProviderFactory.create_provider(provider_name, model_name=model_name)
        except Exception as e:
            print(f"Fehler bei Provider-Initialisierung: {str(e)}")
            raise
        
        # Cache f√ºr Relevanzpr√ºfungen
        self.relevance_cache = {}
        self.relevance_details = {}
        
        # Tracking-Metriken
        self.total_segments = 0
        self.relevant_segments = 0
        self.api_calls = 0

        # Hole Ausschlussregeln aus KODIERREGELN
        self.exclusion_rules = KODIERREGELN.get('exclusion', [])
        print("\nRelevanceChecker initialisiert:")
        print(f"- {len(self.exclusion_rules)} Ausschlussregeln geladen")

        # Hole Mehrfachkodierungsparameter aus CONFIG
        self.multiple_codings_enabled = CONFIG.get('MULTIPLE_CODINGS', True)
        self.multiple_threshold = float(CONFIG.get('MULTIPLE_CODING_THRESHOLD', 0.7))

        # Cache f√ºr Mehrfachkodierungen
        self.multiple_coding_cache = {}
        
        print("\nRelevanceChecker initialisiert:")
        print(f"- {len(self.exclusion_rules)} Ausschlussregeln geladen")
        print(f"- Mehrfachkodierung: {'Aktiviert' if self.multiple_codings_enabled else 'Deaktiviert'}")
        if self.multiple_codings_enabled:
            print(f"- Mehrfachkodierung-Schwellenwert: {self.multiple_threshold}")
        
        # Prompt-Handler
        self.prompt_handler = QCAPrompts(
            forschungsfrage=FORSCHUNGSFRAGE,
            kodierregeln=KODIERREGELN,
            deduktive_kategorien=CONFIG.get('DEDUKTIVE_KATEGORIEN', {})
        )

    def _format_segments_for_batch(self, segments: List[Tuple[str, str]]) -> str:
        """
        Formatiert Segmente f√ºr Batch-Verarbeitung in der Relevanzpr√ºfung
        
        Args:
            segments: Liste von (segment_id, text) Tupeln
            
        Returns:
            str: Formatierte Segmente f√ºr den Prompt
        """
        formatted_segments = []
        for i, (segment_id, text) in enumerate(segments, 1):
            # Begrenze Textl√§nge f√ºr bessere Performance
            truncated_text = text[:800] + "..." if len(text) > 800 else text
            # Entferne problematische Zeichen
            clean_text = truncated_text.replace('\n', ' ').replace('\r', ' ').strip()
            formatted_segments.append(f"SEGMENT {i}:\n{clean_text}\n")
        
        return "\n".join(formatted_segments)
    
    async def check_multiple_category_relevance(self, segments: List[Tuple[str, str]], 
                                            categories: Dict[str, CategoryDefinition]) -> Dict[str, List[Dict]]:
        """
        PARALLELISIERTE VERSION: Pr√ºft ob Segmente f√ºr mehrere Hauptkategorien relevant sind.
        """
        if not self.multiple_codings_enabled:
            return {}
            
        # Filtere bereits gecachte Segmente
        uncached_segments = [
            (sid, text) for sid, text in segments 
            if sid not in self.multiple_coding_cache
        ]
        
        if not uncached_segments:
            return {sid: self.multiple_coding_cache[sid] for sid, _ in segments}

        try:
            print(f"üöÄ Parallele Mehrfachkodierungs-Pruefung: {len(uncached_segments)} Segmente")
            
            # Bereite Kategorien-Kontext vor
            category_descriptions = []
            for cat_name, cat_def in categories.items():
                if cat_name not in ["Nicht kodiert", "Kein Kodierkonsens"]:
                    examples_list = list(cat_def.examples) if isinstance(cat_def.examples, set) else cat_def.examples
                    category_descriptions.append({
                        'name': cat_name,
                        'definition': cat_def.definition[:200] + '...' if len(cat_def.definition) > 200 else cat_def.definition,
                        'examples': examples_list[:2] if examples_list else []
                    })

            # üöÄ PARALLEL: Hilfsfunktion f√ºr einzelnes Segment
            async def check_single_segment_multiple(segment_id: str, text: str) -> Tuple[str, List[Dict]]:
                """Pr√ºft ein einzelnes Segment auf Mehrfachkodierung parallel."""
                try:
                    prompt = self.prompt_handler.get_multiple_category_relevance_prompt(
                        segments_text=f"SEGMENT:\n{text}",
                        category_descriptions=category_descriptions,
                        multiple_threshold=self.multiple_threshold
                    )
                    
                    token_counter.start_request()
                    
                    # Erstelle Parameter-Dict
                    response = await self.llm_provider.create_completion(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": "Du bist ein Experte f√ºr qualitative Inhaltsanalyse. Du antwortest auf deutsch. Antworte ausschliesslich mit einem JSON-Objekt."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=self.temperature,
                        response_format={"type": "json_object"}
                     )
                    
                    llm_response = LLMResponse(response)
                    try:
                        result = json.loads(llm_response.extract_json())
                    except json.JSONDecodeError as e:
                        print(f"‚ÄºÔ∏è JSONDecodeError in check_multiple_category_relevance for segment {segment_id}: {e}")
                        print(f"‚ÄºÔ∏è Raw LLM response: {llm_response.content}")
                        print(f"‚ö†Ô∏è Fallback: Keine Mehrfachkodierung f√ºr dieses Segment")
                        # Fallback: Keine Mehrfachkodierung
                        return segment_id, []
                    
                    
                    token_counter.track_response(response, self.model_name)
                    
                    # Verarbeite Ergebnisse - erwarte single segment format
                    segment_result = result.get('segment_results', [{}])[0] if result.get('segment_results') else result
                    
                    # Filtere Kategorien nach Schwellenwert
                    relevant_categories = []
                    for cat in segment_result.get('relevant_categories', []):
                        if cat.get('relevance_score', 0) >= self.multiple_threshold:
                            relevant_categories.append(cat)
                    
                    return segment_id, relevant_categories
                    
                except Exception as e:
                    print(f"[WARN] Fehler bei Mehrfachkodierungs-Pruefung {segment_id}: {str(e)}")
                    return segment_id, []  # Leer = keine Mehrfachkodierung
            
            # üöÄ Erstelle Tasks f√ºr alle Segmente
            tasks = [
                check_single_segment_multiple(segment_id, text) 
                for segment_id, text in uncached_segments
            ]
            
            # üöÄ F√ºhre alle parallel aus
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            processing_time = time.time() - start_time
            
            # Verarbeite Ergebnisse und aktualisiere Cache
            multiple_coding_results = {}
            successful_checks = 0
            segments_with_multiple = 0
            
            for result in results:
                if isinstance(result, Exception):
                    print(f"‚ö†Ô∏è Ausnahme bei Mehrfachkodierungs-Pr√ºfung: {result}")
                    continue
                
                segment_id, relevant_categories = result
                
                # Cache aktualisieren
                self.multiple_coding_cache[segment_id] = relevant_categories
                multiple_coding_results[segment_id] = relevant_categories
                
                successful_checks += 1
                
                # Debug-Ausgabe f√ºr Mehrfachkodierung
                if len(relevant_categories) > 1:
                    segments_with_multiple += 1
                    print(f"  üîÅ Mehrfachkodierung: {segment_id}")
                    for cat in relevant_categories:
                        print(f"    - {cat['category']}: {cat['relevance_score']:.2f}")
            
            print(f"‚ö° {successful_checks} Mehrfachkodierungs-Pr√ºfungen in {processing_time:.2f}s")
            print(f"üîÑ {segments_with_multiple} Segmente mit Mehrfachkodierung identifiziert")
            
            # Kombiniere mit Cache f√ºr alle urspr√ºnglichen Segmente
            final_results = {}
            for segment_id, text in segments:
                if segment_id in multiple_coding_results:
                    final_results[segment_id] = multiple_coding_results[segment_id]
                else:
                    final_results[segment_id] = self.multiple_coding_cache.get(segment_id, [])
            
            return final_results

        except Exception as e:
            print(f"Fehler bei paralleler Mehrfachkodierungs-Pr√ºfung: {str(e)}")
            import traceback
            traceback.print_exc()
            return {}
    
    
    async def check_relevance_batch(self, segments: List[Tuple[str, str]]) -> Dict[str, bool]:
        """
        Pr√ºft die Relevanz mehrerer Segmente parallel mit Batch-Verarbeitung.
        Beh√§lt die bew√§hrte Batch-Logik bei und f√ºgt Parallelisierung nur f√ºr gro√üe Batches hinzu.
        
        Args:
            segments: Liste von (segment_id, text) Tupeln
            
        Returns:
            Dict[str, bool]: Mapping von segment_id zu Relevanz
        """
        # Filtere bereits gecachte Segmente
        uncached_segments = [
            (sid, text) for sid, text in segments 
            if sid not in self.relevance_cache
        ]
        
        if not uncached_segments:
            return {sid: self.relevance_cache[sid] for sid, _ in segments}

        try:
            print(f"üîç Relevanzpr√ºfung: {len(uncached_segments)} neue Segmente")
            
            # STRATEGIE: Kleine Batches (‚â§5) ‚Üí bew√§hrte Batch-Methode
            #           Gro√üe Batches (>5) ‚Üí Parallelisierung in Sub-Batches
            
            if len(uncached_segments) <= 5:
                # BEW√ÑHRTE BATCH-METHODE f√ºr kleine Gruppen
                print(f"   üì¶ Verwende normale Batch-Methode f√ºr {len(uncached_segments)} Segmente")
                
                # Erstelle formatierten Text f√ºr Batch-Verarbeitung
                segments_text = "\n\n=== SEGMENT BREAK ===\n\n".join(
                    f"SEGMENT {i + 1}:\n{text}" 
                    for i, (_, text) in enumerate(uncached_segments)
                )

                prompt = self.prompt_handler.get_relevance_check_prompt(
                    segments_text=segments_text,
                    exclusion_rules=self.exclusion_rules
                )
                
                token_counter.start_request()
                
                # Ein API-Call f√ºr alle Segmente
                response = await self.llm_provider.create_completion(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "Du bist ein Experte f√ºr qualitative Inhaltsanalyse. Du antwortest auf deutsch."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    response_format={"type": "json_object"}
                )
                
                
                llm_response = LLMResponse(response)
                try:
                    results = json.loads(llm_response.extract_json())
                except json.JSONDecodeError as e:
                    print(f"‚ÄºÔ∏è JSONDecodeError in check_relevance_batch: {e}")
                    print(f"‚ÄºÔ∏è Raw LLM response: {llm_response.content}")
                    print(f"‚ö†Ô∏è Fallback: Markiere alle Segmente als relevant")
                    # Fallback: Alle als relevant, damit Analyse weitergehen kann
                    for segment_id, _ in uncached_segments:
                        self.relevance_cache[segment_id] = True
                    return {sid: True for sid, _ in segments}
                
                
                token_counter.track_response(response, self.model_name)
                
                # Verarbeite Ergebnisse
                relevance_results = {}
                segment_results = results.get('segment_results', [])
                
                if len(segment_results) != len(uncached_segments):
                    print(f"‚ö†Ô∏è Warnung: Anzahl Ergebnisse ({len(segment_results)}) != Anzahl Segmente ({len(uncached_segments)})")
                    # Fallback: Alle als relevant markieren
                    for segment_id, _ in uncached_segments:
                        relevance_results[segment_id] = True
                        self.relevance_cache[segment_id] = True
                else:
                    for i, (segment_id, _) in enumerate(uncached_segments):
                        segment_result = segment_results[i]
                        is_relevant = segment_result.get('is_relevant', True)  # Default: relevant
                        
                        # Cache-Aktualisierung
                        self.relevance_cache[segment_id] = is_relevant
                        # Erweiterte Details-Speicherung mit Begr√ºndung
                        # FIX: Vereinheitlicht auf 'reasoning' (kein 'justification' Duplikat)
                        # 'reasoning' kommt direkt vom LLM und ist die Hauptbegr√ºndung
                        self.relevance_details[segment_id] = {
                            'confidence': segment_result.get('confidence', 0.8),
                            'key_aspects': segment_result.get('key_aspects', []),
                            'reasoning': segment_result.get('reasoning', segment_result.get('justification', 'Keine Begr√ºndung verf√ºgbar')),
                            'is_relevant': is_relevant,
                            'main_themes': segment_result.get('main_themes', []),
                            'exclusion_match': segment_result.get('exclusion_match', False)
                        }
                        
                        relevance_results[segment_id] = is_relevant
                        
                        # Tracking-Aktualisierung
                        self.total_segments += 1
                        if is_relevant:
                            self.relevant_segments += 1
                
                self.api_calls += 1
                
            else:
                # PARALLELISIERUNG f√ºr gro√üe Batches
                print(f"   üöÄ Verwende Parallelisierung in Sub-Batches fuer {len(uncached_segments)} Segmente")
                
                # Teile in Sub-Batches von je 3-4 Segmenten
                sub_batch_size = 3
                sub_batches = [
                    uncached_segments[i:i + sub_batch_size] 
                    for i in range(0, len(uncached_segments), sub_batch_size)
                ]
                
                async def process_sub_batch(sub_batch: List[Tuple[str, str]]) -> Dict[str, bool]:
                    """Verarbeitet einen Sub-Batch mit der bew√§hrten Methode."""
                    try:
                        # Verwende dieselbe Batch-Logik wie oben
                        segments_text = "\n\n=== SEGMENT BREAK ===\n\n".join(
                            f"SEGMENT {i + 1}:\n{text}" 
                            for i, (_, text) in enumerate(sub_batch)
                        )

                        prompt = self.prompt_handler.get_relevance_check_prompt(
                            segments_text=segments_text,
                            exclusion_rules=self.exclusion_rules
                        )
                        
                        token_counter.start_request()
                        
                        response = await self.llm_provider.create_completion(
                            model=self.model_name,
                            messages=[
                                {"role": "system", "content": "Du bist ein Experte f√ºr qualitative Inhaltsanalyse. Du antwortest auf deutsch."},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=self.temperature,
                            response_format={"type": "json_object"}
                        )
                        
                        llm_response = LLMResponse(response)
                        try:
                            results = json.loads(llm_response.extract_json())
                        except json.JSONDecodeError as e:
                            print(f"‚ÄºÔ∏è JSONDecodeError in process_sub_batch: {e}")
                            print(f"‚ÄºÔ∏è Raw LLM response: {llm_response.content}")
                            print(f"‚ö†Ô∏è Fallback f√ºr Sub-Batch: Markiere alle als relevant")
                            # Fallback: Alle als relevant
                            return {segment_id: True for segment_id, _ in sub_batch}
                        
                        
                        token_counter.track_response(response, self.model_name)
                        
                        # Verarbeite Sub-Batch Ergebnisse
                        sub_batch_results = {}
                        segment_results = results.get('segment_results', [])
                        
                        if len(segment_results) != len(sub_batch):
                            # Fallback bei Mismatch
                            for segment_id, _ in sub_batch:
                                sub_batch_results[segment_id] = True
                        else:
                            for i, (segment_id, _) in enumerate(sub_batch):
                                segment_result = segment_results[i]
                                is_relevant = segment_result.get('is_relevant', True)
                                sub_batch_results[segment_id] = is_relevant
                                
                                # Erweiterte Details-Speicherung mit Begr√ºndung
                                # FIX: Vereinheitlicht auf 'reasoning' (kein 'justification' Duplikat)
                                self.relevance_details[segment_id] = {
                                    'confidence': segment_result.get('confidence', 0.8),
                                    'key_aspects': segment_result.get('key_aspects', []),
                                    'reasoning': segment_result.get('reasoning', segment_result.get('justification', 'Keine Begr√ºndung verf√ºgbar')),
                                    'is_relevant': is_relevant,
                                    'main_themes': segment_result.get('main_themes', []),
                                    'exclusion_match': segment_result.get('exclusion_match', False)
                                }
                        
                        return sub_batch_results
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è Fehler in Sub-Batch: {str(e)}")
                        # Fallback: Alle als relevant markieren
                        return {segment_id: True for segment_id, _ in sub_batch}
                
                # F√ºhre alle Sub-Batches parallel aus
                start_time = time.time()
                tasks = [process_sub_batch(sub_batch) for sub_batch in sub_batches]
                sub_batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                processing_time = time.time() - start_time
                
                # Kombiniere Ergebnisse
                relevance_results = {}
                successful_batches = 0
                
                for result in sub_batch_results:
                    if isinstance(result, Exception):
                        print(f"‚ö†Ô∏è Sub-Batch Ausnahme: {result}")
                        continue
                    
                    if isinstance(result, dict):
                        relevance_results.update(result)
                        successful_batches += 1
                
                # Aktualisiere Cache und Tracking
                for segment_id, is_relevant in relevance_results.items():
                    self.relevance_cache[segment_id] = is_relevant
                    self.total_segments += 1
                    if is_relevant:
                        self.relevant_segments += 1
                
                # API-Calls = Anzahl der Sub-Batches
                self.api_calls += successful_batches
                
                print(f"   ‚ö° {successful_batches} Sub-Batches in {processing_time:.2f}s verarbeitet")
            
            # Debug-Ausgabe der Ergebnisse
            relevant_count = sum(1 for is_relevant in relevance_results.values() if is_relevant)
            print(f"   ‚ÑπÔ∏è Relevanz-Ergebnisse: {relevant_count}/{len(relevance_results)} als relevant eingestuft")
            
            # Kombiniere mit bereits gecachten Ergebnissen f√ºr finale Antwort
            final_results = {}
            for segment_id, text in segments:
                if segment_id in relevance_results:
                    final_results[segment_id] = relevance_results[segment_id]
                else:
                    final_results[segment_id] = self.relevance_cache.get(segment_id, True)
            
            return final_results

        except Exception as e:
            print(f"Fehler bei paralleler Relevanzpr√ºfung: {str(e)}")
            import traceback
            traceback.print_exc()
            # Fallback: Alle als relevant markieren bei Fehler
            return {sid: True for sid, _ in segments}
    

    async def check_relevance_with_category_preselection(self, segments: List[Tuple[str, str]], 
                                                        categories: Dict[str, CategoryDefinition] = None,
                                                        mode: str = 'deductive') -> Dict[str, Dict]:
        """
        Erweiterte Relevanzpr√ºfung mit Hauptkategorie-Vorauswahl (nur f√ºr deduktiven Modus)
        
        Returns:
            Dict mit segment_id als Key und Dict mit 'is_relevant', 'preferred_categories', 'relevance_scores'
        """
        if mode != 'deductive' or not categories:
            # Fallback auf Standard-Relevanzpr√ºfung f√ºr andere Modi
            standard_results = await self.check_relevance_batch(segments)
            return {
                sid: {'is_relevant': is_relevant, 'preferred_categories': [], 'relevance_scores': {}}
                for sid, is_relevant in standard_results.items()
            }
        
        print(f"üß´ Erweiterte Relevanzpr√ºfung mit Kategorie-Vorauswahl fuer {len(segments)} Segmente...")
        
        # Cache-Key f√ºr erweiterte Relevanzpr√ºfung
        cache_key_base = "extended_relevance"
        
        results = {}
        segments_to_process = []
        
        # Cache-Pr√ºfung
        for segment_id, text in segments:
            cache_key = f"{cache_key_base}_{hash(text)}"
            if cache_key in self.relevance_cache:
                results[segment_id] = self.relevance_cache[cache_key]
            else:
                segments_to_process.append((segment_id, text))
        
        if not segments_to_process:
            return results
        
        # Batch-Verarbeitung f√ºr nicht-gecachte Segmente
        batch_text = self._format_segments_for_batch(segments_to_process)
        
        prompt = self.prompt_handler.get_relevance_with_category_preselection_prompt(
            batch_text, categories
        )
        
        try:
            self.api_calls += 1
            response = await self.llm_provider.create_completion(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "Du bist ein Experte f√ºr qualitative Inhaltsanalyse. Du antwortest auf deutsch."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                response_format={"type": "json_object"}
            )
            
            llm_response = LLMResponse(response)
            batch_results = json.loads(llm_response.extract_json())
            
            # Verarbeite Batch-Ergebnisse
            segment_results = batch_results.get('segment_results', [])
            
            for i, (segment_id, text) in enumerate(segments_to_process):
                if i < len(segment_results):
                    segment_result = segment_results[i]
                    
                    result = {
                        'is_relevant': segment_result.get('is_relevant', False),
                        'preferred_categories': segment_result.get('preferred_categories', []),
                        'relevance_scores': segment_result.get('relevance_scores', {}),
                        'reasoning': segment_result.get('reasoning', '')
                    }
                    
                    # Cache speichern
                    cache_key = f"{cache_key_base}_{hash(text)}"
                    self.relevance_cache[cache_key] = result
                    
                    results[segment_id] = result
                else:
                    # Fallback bei unvollst√§ndigen Ergebnissen
                    results[segment_id] = {
                        'is_relevant': False,
                        'preferred_categories': [],
                        'relevance_scores': {},
                        'reasoning': 'Unvollst√§ndiges Batch-Ergebnis'
                    }
            
            print(f"‚úÖ Erweiterte Relevanzpr√ºfung abgeschlossen: {len([r for r in results.values() if r['is_relevant']])} relevante Segmente")
            
        except Exception as e:
            print(f"Fehler bei erweiterter Relevanzpr√ºfung: {str(e)}")
            # Fallback auf Standard-Relevanzpr√ºfung
            standard_results = await self.check_relevance_batch(segments_to_process)
            for segment_id, is_relevant in standard_results.items():
                results[segment_id] = {
                    'is_relevant': is_relevant,
                    'preferred_categories': [],
                    'relevance_scores': {},
                    'reasoning': 'Fallback nach Fehler'
                }
        
        return results
    
    def get_relevance_details(self, segment_id: str) -> Optional[Dict]:
        """Gibt detaillierte Relevanzinformationen f√ºr ein Segment zur√ºck."""
        return self.relevance_details.get(segment_id)

    def get_statistics(self) -> Dict:
        """Gibt Statistiken zur Relevanzpr√ºfung zur√ºck."""
        return {
            'total_segments': self.total_segments,
            'relevant_segments': self.relevant_segments,
            'relevance_rate': self.relevant_segments / max(1, self.total_segments),
            'api_calls': self.api_calls,
            'cache_size': len(self.relevance_cache)
        }
